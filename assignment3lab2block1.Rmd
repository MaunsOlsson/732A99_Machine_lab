---
title: "Assignment 3 lab 2 block 1"
author: "Paripurna Bawonoputro"
date: "2025-11-27"
output: pdf_document
---

```{r setup, include=FALSE}
library(knitr)
library(dplyr)
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 3 Principal components and implicit regularization

First we have to load the data from communities.csv

```{r import communities data, include=FALSE}
data <- read.csv("communities.csv", header = TRUE)
```

## Task 1

In this task, we're not asked to divide the data to train/test.

First, we need to separate the features and the target
Then we scale the features
Implement PCA using eigen() and computing variation explained

```{r separate features and target then scale features, include=FALSE}
X <- subset(data, select = -ViolentCrimesPerPop)

y <- data$ViolentCrimesPerPop

X_scaled <- scale(X)
```



Implement PCA using eigen() and computing variation explained
```{r eigen decomposition, echo=FALSE}
S <- cov(X_scaled)
eigen_decomp <- eigen(S)

eigenvalues <- eigen_decomp$values


var_explained <- eigenvalues / sum(eigenvalues)

cum_var <- cumsum(var_explained)

# Create variance explained table
pc_table <- data.frame(
  PC = 1:length(var_explained),
  Variance_Explained = var_explained,
  Cumulative = cum_var
)
rownames(pc_table) <- NULL

block1 <- pc_table[1:20, ]
block2 <- pc_table[21:40, ]

combined <- data.frame(
  PC_1to20 = block1$PC,
  VarExp_1to20 = round(block1$Variance_Explained, 4),
  CumVar_1to20 = round(block1$Cumulative, 4),
  
  PC_21to40 = block2$PC,
  VarExp_21to40 = round(block2$Variance_Explained, 4),
  CumVar_21to40 = round(block2$Cumulative, 4)
)

kable(combined, caption = "Variance Explained by Principal Components (1–40)")
```

Based on Table 1, components needed to obtain at least 95% of variance in the data is
```{r higher than 95 percent variation explained, echo=FALSE}
which(cum_var >= 0.95)[1]
```

The proportion of variation explained by PC1 and PC2 based on Table 1 are
```{r variation of pc1 and pc2, echo=FALSE}
var_explained[1:2]
```


## Task 2

```{r Repeating PCA using princomp, include=FALSE}
pca2 <- princomp(X_scaled, cor = FALSE)

```

```{r Trace plotting the first principal component, echo=FALSE}
pc1_loadings <- pca2$loadings[,1]

plot(pc1_loadings, type = "b",
     main = "Trace Plot of PC1 Loadings",
     xlab = "Feature Index",
     ylab = "Loading Value")
```

Many loadings are close to 0, showing several variables do not contribute strongly to PC1.

PC1 is influenced by many moderately strong variables rather than a few dominant ones.

Both positive and negative contributions appear, means some features are positively associated with the PC1 direction while other features are negatively associated.


The five variables with the highest loadings on PC1 are:

```{r Finding most contributing features, echo=FALSE}
pc1 <- pca2$loadings[,1]
top5_idx <- order(abs(pc1), decreasing = TRUE)[1:5]
colnames(X_scaled)[top5_idx]
pc1[top5_idx]
```


medFamInc        (Median Family Income) negative\
medIncome        (Median Household Income) negative\
PctKids2Par      (Percentage of kids in family housing with two parents) negative\
pctWInvInc       (Percentage of households with investment / rent income in 1989) negative\
PctPopUnderPov   (Percentage of people under the poverty level) positive\

PC1 is primarily driven by income variables and family structure, with high positive loading from poverty level. Communities with higher income and more two-parent households load negatively on PC1, while poorer communities load positively. In other words, place where the socio-economic status higher are less likely to have high crime records and vice versa.

```{r Plot PC scores, echo=FALSE}
pca2 <- princomp(X_scaled)

scores <- pca2$scores

library(ggplot2)

df_plot <- data.frame(
  PC1 = scores[,1],
  PC2 = scores[,2],
  Crime = data$ViolentCrimesPerPop
)

ggplot(df_plot, aes(x = PC1, y = PC2, color = Crime)) +
  geom_point(alpha = 0.7, size = 2) +
  scale_color_gradient(low = "blue", high = "red") +
  theme_minimal() +
  labs(
    title = "PC1 vs PC2 Scores Colored by Violent Crime Rate",
    x = "PC1",
    y = "PC2",
    color = "Crime Rate"
  )
```

The PC1–PC2 score plot shows that the violent crime rate increases along PC1. Communities on the right side of PC1 have higher crime and correspond to high-poverty, low-income areas identified in the loading analysis. Low-crime communities appear on the left side, corresponding to higher-income and more stable neighborhoods. PC2 shows less association with crime, meaning PC1 is the primary component capturing the socioeconomic factors driving crime variation


## Task 3
Split data into training and test. Then separate the features and the target.

```{r split train test, echo=FALSE}
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]

X_train <- subset(train, select = -ViolentCrimesPerPop)
y_train <- train$ViolentCrimesPerPop

X_test <- subset(test, select = -ViolentCrimesPerPop)
y_test <- test$ViolentCrimesPerPop
```

Then we have to apply scale to the feature and target. We should only use the parameter of train data to scale both train and test data. The actual code can be found in the appendix.
```{r scale the data, echo=FALSE}

X_means <- apply(X_train, 2, mean)
X_sd   <- apply(X_train, 2, sd)

y_mean <- mean(y_train)
y_sd   <- sd(y_train)

X_train_scaled <- scale(X_train, center = X_means, scale = X_sd)
y_train_scaled <- (y_train - y_mean) / y_sd

X_test_scaled <- scale(X_test, center = X_means, scale = X_sd)
y_test_scaled <- (y_test - y_mean) / y_sd

```

Then we fit the model and use it for prediction
```{r fit and predict, echo=FALSE}
linear_model <- lm(y_train_scaled ~ X_train_scaled)

train_pred <- predict(linear_model, newdata = as.data.frame(X_train_scaled))
test_pred <- predict(linear_model, newdata = as.data.frame(X_test_scaled))
```

To evaluate the fitted model, we compute the **mean squared error (MSE)** and the
**coefficient of determination** $R^2$  for both the training and test sets.

The MSE is defined as

$$ MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 $$

which measures the average squared difference between the observed target values
and the model predictions. Lower MSE indicates better predictive accuracy.

The $R^2$ score is computed as

$$
R^2 = 1 - 
\frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}
     {\sum_{i=1}^n (y_i - \bar{y})^2},
$$

where $\bar{y}$ is the mean of the true target values.  
An $R^2$ close to 1 indicates a strong model fit, while a **negative** $R^2$
means the model performs **worse than simply predicting the mean** of the target.

```{r calculate MSE, echo=FALSE}
train_mse <- mean((y_train_scaled - train_pred)^2)
test_mse <- mean((y_test_scaled - test_pred)^2)

train_r2 <- 1 - sum((y_train_scaled - train_pred)^2) / sum((y_train_scaled - mean(y_train_scaled))^2)
test_r2 <- 1 - sum((y_test_scaled - test_pred)^2) / sum((y_test_scaled - mean(y_test_scaled))^2)

library(knitr)

results <- data.frame(
  Dataset = c("Train", "Test"),
  MSE = c(train_mse, test_mse),
  R2 = c(train_r2, test_r2)
)

kable(results, digits = 4,
      caption = "Model Performance on Training and Test Sets")

```

The linear regression model shows good performance on the training set. However, the test set performance is very poor ($R^2$ = -0.62), meaning the model predicts worse than a simple mean-based model. The large discrepancy between training and test errors indicates overfitting. This concludes that the model is not suitable for predictive purposes in its current form. One of the possible reasons is there are too many predictors relative to sample size.  If this is the case, feature selection may be needed to improve generalization.

## Task 4

The cost function for linear regression without intercept is:
$$J(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - X_i \theta)^2$$


```{r Define cost function, echo=FALSE}
cost_fn <- function(theta, X, y) {
  preds <- X %*% theta
  mean((y - preds)^2)
}
```


```{r optimize theta, include=FALSE}
train_errors <- c()
test_errors <- c()

cost_with_tracking <- function(theta) {
  # Compute training error
  train_err <- cost_fn(theta, X_train_scaled, y_train_scaled)
  test_err <- cost_fn(theta, X_test_scaled, y_test_scaled)
  
  # Store errors
  train_errors <<- c(train_errors, train_err)
  test_errors <<- c(test_errors, test_err)
  
  return(train_err)
}

init_theta <- rep(0, ncol(X_train_scaled))
result <- optim(par = init_theta, fn = cost_with_tracking, method = "BFGS", control = list(trace = 1, maxit = 2000))
```

Plot the errors

```{r Plot errors vs iteration, echo=FALSE}


optimal_iter <- which.min(test_errors)

start <- 1         
end   <- 5000  


idx <- start:min(end, length(train_errors))

plot(idx,
     train_errors[idx],
     type = "l", col = "blue", lwd = 2, log = "y",
     xlim = c(start, end),
     ylim = range(c(train_errors[idx], test_errors[idx])),
     xlab = "Function Evaluations", ylab = "Error (log)",
     main = "Training vs Test Error during BFGS Optimization")

lines(idx, test_errors[idx], col = "red", lwd = 2)

legend("topright",
       legend = c("Train", "Test"),
       col = c("blue", "red"),
       lty = 1, lwd = 2)


if (optimal_iter >= start && optimal_iter <= end) {
  points(optimal_iter, test_errors[optimal_iter],
         col = "darkgreen", pch = 19, cex = 1.1)
}

```

Find optimal iteration with minumum error

```{r Find optimal iteration, echo=FALSE}

cat("Optimal iteration:", optimal_iter, "Test error:", test_errors[optimal_iter], "\n")

```

The BFGS optimization successfully minimized the training cost, with training error approaching zero. 
The optimization achieved a much lower test error ($\approx$ 0.40) compared to the linear regression in Step 3 ($\approx$ 1.81). This indicates that the iterative approach provided better prediction. The iteration after the optimum might decrease the error for training data but will increase the error for test data due to overfitting.

# 4. Theory
## Part 3
According to the MLFC book page 101-102, imbalanced data can be handled either by *modifying the loss function* or *by modifying the training data*. To implement modification of the loss function, we can use cost-sensitive loss, where errors on the minority class are *penalised C-times more heavily*. We can also modify the data instead, by *duplicating minority class* samples C times in the training set.



# APPENDIX 3

First we have to load the data from communities.csv

```{r import communities data appendix, eval=FALSE}
data <- read.csv("communities.csv", header = TRUE)
```

## Task 1

In this task, we're not asked to divide the data to train/test.

First, we need to separate the features and the target
Then we scale the features
Implement PCA using eigen() and computing variation explained

```{r separate features and target then scale features appendix, eval=FALSE}
X <- subset(data, select = -ViolentCrimesPerPop)

y <- data$ViolentCrimesPerPop

X_scaled <- scale(X)
```



Implement PCA using eigen() and computing variation explained
```{r eigen decomposition appendix, eval=FALSE}
S <- cov(X_scaled)
eigen_decomp <- eigen(S)

eigenvalues <- eigen_decomp$values


var_explained <- eigenvalues / sum(eigenvalues)

cum_var <- cumsum(var_explained)

# Create variance explained table
pc_table <- data.frame(
  PC = 1:length(var_explained),
  Variance_Explained = var_explained,
  Cumulative = cum_var
)
rownames(pc_table) <- NULL

block1 <- pc_table[1:20, ]
block2 <- pc_table[21:40, ]

combined <- data.frame(
  PC_1to20 = block1$PC,
  VarExp_1to20 = round(block1$Variance_Explained, 4),
  CumVar_1to20 = round(block1$Cumulative, 4),
  
  PC_21to40 = block2$PC,
  VarExp_21to40 = round(block2$Variance_Explained, 4),
  CumVar_21to40 = round(block2$Cumulative, 4)
)

kable(combined, caption = "Variance Explained by Principal Components (1–40)")
```

Components needed to obtain at least 95% of variance in the data are
```{r higher than 95 percent variation explained appendix, eval=FALSE}
which(cum_var >= 0.95)[1]
```

The proportion of variation explained by PC1 and PC2 are
```{r variation of pc1 and pc2 appedix, eval=FALSE}
var_explained[1:2]
```


## Task 2

```{r Repeating PCA using princomp appendix, eval=FALSE}
pca2 <- princomp(X_scaled, cor = FALSE)

```

```{r Trace plotting the first principal component appendix, eval=FALSE}
pc1_loadings <- pca2$loadings[,1]

plot(pc1_loadings, type = "b",
     main = "Trace Plot of PC1 Loadings",
     xlab = "Feature Index",
     ylab = "Loading Value")
```

The five variables with the highest loadings on PC1 are:

```{r Finding most contributing features appendix, eval=FALSE}
pc1 <- pca2$loadings[,1]
top5_idx <- order(abs(pc1), decreasing = TRUE)[1:5]
colnames(X_scaled)[top5_idx]
pc1[top5_idx]
```

Plot

```{r Plot PC scores appendix, eval=FALSE}
pca2 <- princomp(X_scaled)

scores <- pca2$scores

library(ggplot2)

df_plot <- data.frame(
  PC1 = scores[,1],
  PC2 = scores[,2],
  Crime = data$ViolentCrimesPerPop
)

ggplot(df_plot, aes(x = PC1, y = PC2, color = Crime)) +
  geom_point(alpha = 0.7, size = 2) +
  scale_color_gradient(low = "blue", high = "red") +
  theme_minimal() + xlim(0, 10000)
  labs(
    title = "PC1 vs PC2 Scores Colored by Violent Crime Rate",
    x = "PC1",
    y = "PC2",
    color = "Crime Rate"
  )
```


## Task 3
Split data into training and test. Then separate the features and the target.

```{r split train test appendix, eval=FALSE}
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]

X_train <- subset(train, select = -ViolentCrimesPerPop)
y_train <- train$ViolentCrimesPerPop

X_test <- subset(test, select = -ViolentCrimesPerPop)
y_test <- test$ViolentCrimesPerPop
```

Then we have to apply scale to the feature and target. We should only use the parameter of train data to scale both train and test data.

```{r scale the data appendix, eval=FALSE}

X_means <- apply(X_train, 2, mean)
X_sd   <- apply(X_train, 2, sd)

y_mean <- mean(y_train)
y_sd   <- sd(y_train)

X_train_scaled <- scale(X_train, center = X_means, scale = X_sd)
y_train_scaled <- (y_train - y_mean) / y_sd

X_test_scaled <- scale(X_test, center = X_means, scale = X_sd)
y_test_scaled <- (y_test - y_mean) / y_sd

```

Then we fit the model and use it for prediction

```{r fit and predict appendix, eval=FALSE}
linear_model <- lm(y_train_scaled ~ X_train_scaled)

train_pred <- predict(linear_model, newdata = as.data.frame(X_train_scaled))
test_pred <- predict(linear_model, newdata = as.data.frame(X_test_scaled))
```



```{r calculate MSE appendix, eval=FALSE}
train_mse <- mean((y_train_scaled - train_pred)^2)
test_mse <- mean((y_test_scaled - test_pred)^2)

train_r2 <- 1 - sum((y_train_scaled - train_pred)^2) / sum((y_train_scaled - mean(y_train_scaled))^2)
test_r2 <- 1 - sum((y_test_scaled - test_pred)^2) / sum((y_test_scaled - mean(y_test_scaled))^2)

library(knitr)

results <- data.frame(
  Dataset = c("Train", "Test"),
  MSE = c(train_mse, test_mse),
  R2 = c(train_r2, test_r2)
)

kable(results, digits = 4,
      caption = "Model Performance on Training and Test Sets")

```


## Task 4

The cost function for linear regression without intercept is:
$$J(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - X_i \theta)^2$$


```{r Define cost function  appendix, eval=FALSE}
cost_fn <- function(theta, X, y) {
  preds <- X %*% theta
  mean((y - preds)^2)
}
```


```{r optimize theta appendix, eval=FALSE}
train_errors <- c()
test_errors <- c()

cost_with_tracking <- function(theta) {
  # Compute training error
  train_err <- cost_fn(theta, X_train_scaled, y_train_scaled)
  test_err <- cost_fn(theta, X_test_scaled, y_test_scaled)
  
  # Store errors
  train_errors <<- c(train_errors, train_err)
  test_errors <<- c(test_errors, test_err)
  
  return(train_err)
}

init_theta <- rep(0, ncol(X_train_scaled))
result <- optim(par = init_theta, fn = cost_with_tracking, method = "BFGS", control = list(trace = 1, maxit = 2000))
```

Plot the errors

```{r Plot errors vs iteration appendix, eval=FALSE}


optimal_iter <- which.min(test_errors)

start <- 1         
end   <- 5000  


idx <- start:min(end, length(train_errors))

plot(idx,
     train_errors[idx],
     type = "l", col = "blue", lwd = 2, log = "y",
     xlim = c(start, end),
     ylim = range(c(train_errors[idx], test_errors[idx])),
     xlab = "Function Evaluations", ylab = "Error (log)",
     main = "Training vs Test Error during BFGS Optimization")

lines(idx, test_errors[idx], col = "red", lwd = 2)

legend("topright",
       legend = c("Train", "Test"),
       col = c("blue", "red"),
       lty = 1, lwd = 2)


if (optimal_iter >= start && optimal_iter <= end) {
  points(optimal_iter, test_errors[optimal_iter],
         col = "darkgreen", pch = 19, cex = 1.1)
}


```

Find optimal iteration with minumum error

```{r Find optimal iteration appendix, eval=FALSE}

cat("Optimal iteration:", optimal_iter, "Test error:", test_errors[optimal_iter], "\n")

```
